# -*- coding: utf-8 -*-
"""CCFD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h7YzDTS_P616-9ayIEQWjp7wK3LlnwhQ
"""

# Imported Libraries
import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.pipeline import make_pipeline
from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import NearMiss
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import RandomOverSampler
from imblearn.metrics import classification_report_imbalanced
from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report, confusion_matrix
from collections import Counter
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

"""## Load and Read Dataset"""

!gdown --id 1lbqX5T7LvKRe0xpes0Ow3Mh9tiBtmC8H

!unzip Fraud_Detection.zip

df = pd.read_csv('application_data.csv')

"""## Dataset Description and Preparation"""

df.head(3)

df.shape

for types in df.dtypes.unique():
    print(types)
    print(df.select_dtypes(types).columns)

#The fraud Vs normal trasaction by hour
plt.figure(figsize=(15,5))
sns.distplot(df[df['TARGET'] == 0]["AMT_CREDIT"], color='green')
sns.distplot(df[df['TARGET'] == 1]["AMT_CREDIT"], color='red')
plt.title('Fraud Vs Normal Transactions by AMT_CREDIT', fontsize=17)
plt.show()

plt.figure(figsize=(15,5))
sns.distplot(df[df['TARGET'] == 0]["LIVINGAPARTMENTS_MODE"], color='green')
sns.distplot(df[df['TARGET'] == 1]["LIVINGAPARTMENTS_MODE"], color='red')
plt.title('Fraud Vs Normal Transactions by LIVINGAPARTMENTS_MODE', fontsize=17)
plt.show()

plt.figure(figsize=(15,5))
sns.distplot(df[df['TARGET'] == 0]["OWN_CAR_AGE"], color='green')
sns.distplot(df[df['TARGET'] == 1]["OWN_CAR_AGE"], color='red')
plt.title('Fraud Vs Normal Transactions by OWN_CAR_AGE', fontsize=17)
plt.show()

plt.figure(figsize=(15,5))
sns.distplot(df[df['TARGET'] == 0]["ENTRANCES_MEDI"], color='green')
sns.distplot(df[df['TARGET'] == 1]["ENTRANCES_MEDI"], color='red')
plt.title('Fraud Vs Normal Transactions by ENTRANCES_MEDI', fontsize=17)
plt.show()

plt.figure(figsize=(15,5))
sns.distplot(df[df['TARGET'] == 0]["DAYS_REGISTRATION"], color='green')
sns.distplot(df[df['TARGET'] == 1]["DAYS_REGISTRATION"], color='red')
plt.title('Fraud Vs Normal Transactions by DAYS_REGISTRATION', fontsize=17)
plt.show()

# visualization null values using heatmap
plt.figure(figsize=(15,5))
sns.heatmap(df.isnull(), cmap = 'plasma', annot=False,yticklabels= False)
plt.title('Visualising missing values')

#Check the fraud/Non_Fraud related records
count=df['TARGET'].value_counts()
count.plot(kind = 'bar')

"""## Spliting Data to Train/Test 

"""

## Splitting the Dataset into Train & Test
x = df.drop('TARGET',axis=1)
y = df['TARGET']
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25,random_state=0)

print('Shape of X_train : ', x_train.shape)
print('Shape of Y_train : ', y_train.shape)
print('Shape of X_test : ', x_test.shape)
print('Shape of Y_test : ', y_test.shape)

"""## Encoding Categorial Values"""

x_train = pd.get_dummies(x_train, columns = ['OCCUPATION_TYPE','NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',
       'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 
       'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE','FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE',
       'EMERGENCYSTATE_MODE'])
x_test = pd.get_dummies(x_test, columns = ['OCCUPATION_TYPE','NAME_CONTRACT_TYPE', 'CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY',
       'NAME_TYPE_SUITE', 'NAME_INCOME_TYPE', 'NAME_EDUCATION_TYPE',
       'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE', 
       'WEEKDAY_APPR_PROCESS_START', 'ORGANIZATION_TYPE','FONDKAPREMONT_MODE', 'HOUSETYPE_MODE', 'WALLSMATERIAL_MODE',
       'EMERGENCYSTATE_MODE'])

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')
X_train = imputer.fit_transform(x_train)
X_test= imputer.fit_transform(x_test)

"""## Checking Class Imbalance"""

classes_train=y_train.value_counts()
normal_share_train=classes_train[0]/y_train.count()*100
fraud_share_train=classes_train[1]/y_train.count()*100


plt.subplot(2,1,1)
plt.bar(['Non-Fraud','Fraud'], classes_train, color=['blue','lightblue'])
plt.title('Train')
plt.ylabel('Number of transactions')
plt.annotate("{0:.4}%".format(normal_share_train),(0.2, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(fraud_share_train),(0.7, 0.5), xycoords='axes fraction')

"""## Oversampling """

## oversampling technique for a more balanced dataset to feed our model
ros = RandomOverSampler(random_state=0)
X_train, Y_train = ros.fit_resample(X_train, y_train)

## Checking Class Imbalance after Sampling

classes_train=Y_train.value_counts()
normal_share_train=classes_train[0]/Y_train.count()*100
fraud_share_train=classes_train[1]/Y_train.count()*100


plt.subplot(2,1,1)
plt.bar(['Non-Fraud','Fraud'], classes_train, color=['blue','lightblue'])
plt.title('Train')
plt.ylabel('Number of transactions')
plt.annotate("{0:.4}%".format(normal_share_train),(0.2, 0.5), xycoords='axes fraction')
plt.annotate("{0:.4}%".format(fraud_share_train),(0.7, 0.5), xycoords='axes fraction')

X_train.shape

X_test.shape

"""## Random Forest"""

random_forest = RandomForestClassifier(random_state=0)
random_forest.fit(X_train,Y_train)
y_pred = random_forest.predict(X_test)

print('Confustion Matrix : \n\n', confusion_matrix(y_test,y_pred))
print('\n Accuracy Score : ',   accuracy_score(y_test,y_pred))
print('\n Classification Report : \n \n', classification_report(y_test,y_pred))

plt.figure(figsize=(20,10))
sns.heatmap(confusion_matrix(y_test,y_pred),annot = True,cmap='icefire')

"""## Threshold"""

threshold0= 0.5
threshold1 = 0.4
threshold2 = 0.3
threshold3 = 0.6
threshold4 = 0.1
threshold5 = 0.7
threshold6 = 0.8

predicted = random_forest.predict_proba(X_test)
predicted0 = (predicted[:,1] >= threshold0).astype('int')
predicted1 = (predicted[:,1] >= threshold1).astype('int')
predicted2 = (predicted[:,1] >= threshold2).astype('int')
predicted3 = (predicted[:,1] >= threshold3).astype('int')
predicted4 = (predicted[:,1] >= threshold4).astype('int')
predicted5 = (predicted[:,1] >= threshold5).astype('int')
predicted6 = (predicted[:,1] >= threshold6).astype('int')

accuracy0 = accuracy_score(y_test, predicted0)
accuracy1 = accuracy_score(y_test, predicted1)
accuracy2 = accuracy_score(y_test, predicted2)
accuracy3 = accuracy_score(y_test, predicted3)
accuracy4 = accuracy_score(y_test, predicted4)
accuracy5 = accuracy_score(y_test, predicted5)
accuracy6 = accuracy_score(y_test, predicted6)

# initialize list of lists
acc= [['0.5',accuracy0],['0.4',accuracy1],['0.6',accuracy2],['0.2',accuracy3] ,['0.1',accuracy4 ],['0.7',accuracy5 ],['0.8',accuracy6 ]]
  
# Create the pandas DataFrame
thr= pd.DataFrame(acc, columns=['Threshold', 'Accuracy'])
  
# print dataframe.
thr

sns.set()
plt.title('Algorithms accuracy of different threshold')
sns.barplot(y="Accuracy", x="Threshold", data =thr, palette="Set2_r")
sns.set(rc={'figure.figsize':(15,5)})

from sklearn.metrics import average_precision_score, precision_recall_curve
pred_prob = random_forest.predict_proba(X_test)
y_score = pred_prob[:,1]
average_precision = average_precision_score(y_test, y_score)
original_precision, original_recall, original_thresholds = precision_recall_curve(y_test, y_score)
plt.step(original_recall, original_precision, color='red', alpha=0.5,linewidth=1.5,label='Original logistic')
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall curve')
plt.legend(loc='upper left', bbox_to_anchor=(1,1), fontsize = 'large')

"""## Features Selection"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif

## taken best 10 features in the model
fs = SelectKBest(score_func=f_classif, k=10) 
model= fs.fit(X_train, Y_train)

#Transforming data with 10 features
X_train_2 = model.transform(X_train)
X_test_2 = model.transform(X_test)
random_forest.fit(X_train_2,Y_train)
pred_2 = random_forest.predict(X_test_2)

X_test_2.shape

print(classification_report(y_test,pred_2))
print(accuracy_score(y_test,pred_2))

##
x_train=x_train.replace(np.nan,0.0)
x_test=x_test.replace(np.nan,0.0)

rf2=RandomForestClassifier()
rf2.fit(x_train,y_train)
feature_importance = rf2.feature_importances_
# importance_df = pd.DataFrame({'features': x_train.columns,
#                               'importance': feature_importance})
# importance_df.sort_values(by='importance', ascending=False, inplace=True)
# importance_df

feature_importance = rf2.feature_importances_
importance_df = pd.DataFrame({'features': x_train.columns,
                              'importance': feature_importance})
importance_df.sort_values(by='importance', ascending=False, inplace=True)
importance_df

feature_list = importance_df['features'].head(10).tolist()

X_train_new = x_train[feature_list]
X_test_new = x_test[feature_list]

rf3=RandomForestClassifier()
rf3.fit(X_train_new,y_train)
pred_new=rf3.predict(X_test_new)

print(classification_report(y_test,pred_new))
print(accuracy_score(y_test,pred_new))